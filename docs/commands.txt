

## Spark Setup on spark nodes
- Setup JAVA
    sudo apt-get update
    sudo apt-get install openjdk-8-jdk
- Setup network between machines. See `tools/p2p_setup.sh` for direct connection.
- Download and build spark. 
    ./build/mvn -Pyarn -Dhadoop.version=2.8.5 -DskipTests clean package
- Using Hadoop 2.8.5. Download and unpack it.
- Follow steps at `docs/setup.txt` to setup HDFS, YARN and Spark.
    - HDFS Commands: 
        start-dfs.sh
        stop-dfs.sh
        hdfs dfsadmin -report
        Web UI at: http://spark-29.sysnet.ucsd.edu:50070
    - YARN commands: 
        start-yarn.sh
        stop-yarn.sh
	    yarn node -list
	    yarn application -list
	    Web UI at http://spark-29.sysnet.ucsd.edu:8088

## Build a Spark application 
- Copied spark-sort from previous experience
- It needs to be built with local spark jars since you will be editing spark
    - build local spark with "sbt package"
    - publish jars to local maven repository using "sbt publishLocal" or "mvn install"
        /build/mvn -Pyarn -Dhadoop.version=2.8.5 -DskipTests clean install
- Edit build.sbt in target application to use proper local version 
    - Add local maven as one of the resolvers
    - "resolvers += Resolver.mavenLocal"


## Adding a new node checklist
- create and mount ramdisk thru /etc/fstab file
- increase memory locked size in /etc/security/limits.conf
- edits /etc/sudoers for automatic sudo permissions


