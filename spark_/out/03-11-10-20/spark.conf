# Spark home
hibench.spark.home      /home/ayelam/spark-nic/spark_/spark

# Spark master
#   standalone mode: spark://xxx:7077
#   YARN mode: yarn-client
hibench.spark.master    yarn

# executor number and cores when running on Yarn
hibench.yarn.executor.num     2
hibench.yarn.executor.cores   6

# executor and driver memory in standalone & YARN mode
spark.executor.memory   30g
spark.driver.memory     5g

# needed for xgboot
# spark.task.cpus         6

# set spark parallelism property according to hibench's parallelism value
spark.default.parallelism     ${hibench.default.map.parallelism}

# set spark sql's default shuffle partitions according to hibench's parallelism value
spark.sql.shuffle.partitions  ${hibench.default.shuffle.parallelism}

#======================================================
# Spark Streaming
#======================================================
# Spark streaming Batchnterval in millisecond (default 100)
hibench.streambench.spark.batchInterval          100

# Number of nodes that will receive kafka input (default: 4)
hibench.streambench.spark.receiverNumber        4

# Indicate RDD storage level. (default: 2)
# 0 = StorageLevel.MEMORY_ONLY
# 1 = StorageLevel.MEMORY_AND_DISK_SER
# other = StorageLevel.MEMORY_AND_DISK_SER_2
hibench.streambench.spark.storageLevel 2

# indicate whether to test the write ahead log new feature (default: false)
hibench.streambench.spark.enableWAL false

# if testWAL is true, this path to store stream context in hdfs shall be specified. If false, it can be empty (default: /var/tmp)
hibench.streambench.spark.checkpointPath /var/tmp

# whether to use direct approach or not (dafault: true)
hibench.streambench.spark.useDirectMode true


# # Custom spark settings
# #
                                         
spark.record.dir /home/ayelam/spark-nic/spark_/tmp/03-11-10-20/
# spark.record.shuffles               0,1,2
# spark.record.maps                   10,11,12,13                          

# To print RDD lineage in logs when a spark action is encountered
spark.logLineage			        true

# Use an efficient java serialization
spark.serializer        		    org.apache.spark.serializer.KryoSerializer

# logs for history server
spark.eventLog.enabled  		    true
spark.eventLog.dir 			        file:/home/ayelam/spark-nic/spark_/tmp/logs

# Switching to G1GC Java Garbage Collector
spark.executor.extraJavaOptions	    -Djol.tryWithSudo=true -Djdk.attach.allowAttachSelf
#-XX:+PrintTLAB -XX:TLABSize=10000000
# spark.executor.extraJavaOptions	-XX:+UseG1GC -XX:+PrintGC -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps
# spark.executor.extraJavaOptions   -Xms150g -XX:+UseNUMA -Xmn120g -XX:+UseParallelGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps  
# -XX:+PrintReferenceGC -verbose:gc -XX:+PrintAdaptiveSizePolicy

# Use spark/sbin/start(stop)-historyserver and navigate to below port to explore spark applications
spark.history.provider            	org.apache.spark.deploy.history.FsHistoryProvider
spark.history.fs.logDirectory     	file:/home/ayelam/spark-nic/spark_/tmp/logs
spark.history.fs.update.interval  	10s
spark.history.ui.port             	18080

# For spark sql
# spark.yarn.jars                     /home/ayelam/spark-nic/spark_/spark/hive-jars/